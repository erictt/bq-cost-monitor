config {
  type: "table",
  description: "Enhanced table containing BigQuery usage and cost data with granular insights",
  columns: {
    date: "Date of the query execution (YYYY-MM-DD)",
    project_id: "Google Cloud project ID",
    user_email: "Email of the user who executed the query",
    service_account: "Service account identifier if the query was executed by a service account",
    query_count: "Number of queries executed",
    cache_hit_count: "Number of queries that used cached results",
    error_count: "Number of queries that resulted in errors",
    total_bytes_processed: "Total bytes processed by all queries",
    total_bytes_billed: "Total bytes billed for all queries",
    estimated_cost_usd: "Estimated cost in USD based on bytes billed",
    slot_hours: "Total slot hours consumed",
    cache_hit_percentage: "Percentage of queries that used cached results",
    hourly_breakdown: "Array containing query count and cost breakdowns by hour of day",
    daily_breakdown: "Array containing query count and cost breakdowns by day of week",
    dataset_costs: "Array of datasets used with their associated costs",
    recent_queries: "Array of recent queries with detailed information for analysis"
  },
  bigquery: {
    partitionBy: "date",
    clusterBy: ["project_id", "user_email"]
  },
  tags: ["cost", "monitoring", "service-accounts", "datasets", "analytics"]
}

-- This query uses the cost_query.sql file with parameters
WITH job_stats AS (
  SELECT
    project_id,
    user_email,
    -- Extract service account info from user_email if present
    CASE 
      WHEN user_email LIKE '%.gserviceaccount.com' THEN user_email
      WHEN user_email LIKE 'service-%' THEN user_email
      ELSE NULL
    END AS service_account,
    job_id,
    creation_time,
    end_time,
    query,
    total_bytes_processed,
    total_bytes_billed,
    total_slot_ms,
    error_result,
    cache_hit,
    destination_table,
    referenced_tables,
    labels,
    -- Extract dataset information from referenced tables
    ARRAY(
      SELECT DISTINCT 
        CONCAT(ref_table.project_id, '.', ref_table.dataset_id)
      FROM 
        UNNEST(referenced_tables) AS ref_table
      WHERE 
        ref_table.project_id IS NOT NULL 
        AND ref_table.dataset_id IS NOT NULL
    ) AS referenced_datasets
  FROM
    ${ref("INFORMATION_SCHEMA.JOBS")}
  WHERE
    creation_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL ${dataform.projectConfig.vars.history_days} DAY)
    AND job_type = 'QUERY'
    AND statement_type != 'SCRIPT'
),

-- Calculate per-dataset costs by attributing the cost of each query proportionally
dataset_costs AS (
  SELECT
    FORMAT_TIMESTAMP('%Y-%m-%d', creation_time) AS date,
    project_id,
    user_email,
    service_account,
    dataset,
    -- Count queries referencing this dataset
    COUNT(*) AS query_count,
    -- Sum bytes processed, attributed proportionally if multiple datasets are involved
    SUM(total_bytes_processed / ARRAY_LENGTH(referenced_datasets)) AS bytes_processed,
    SUM(total_bytes_billed / ARRAY_LENGTH(referenced_datasets)) AS bytes_billed,
    -- Calculate approximate dataset cost (using the configurable cost per TB)
    ROUND(SUM(total_bytes_billed / ARRAY_LENGTH(referenced_datasets)) / 
          POWER(1024, 4) * ${dataform.projectConfig.vars.cost_per_terabyte}, 2) AS dataset_cost_usd
  FROM
    job_stats,
    UNNEST(referenced_datasets) AS dataset
  WHERE
    ARRAY_LENGTH(referenced_datasets) > 0
  GROUP BY
    date, project_id, user_email, service_account, dataset
)

-- Main query with user and service account attribution
SELECT
  FORMAT_TIMESTAMP('%Y-%m-%d', js.creation_time) AS date,
  js.project_id,
  js.user_email,
  js.service_account,
  -- Query stats
  COUNT(*) AS query_count,
  SUM(CASE WHEN js.cache_hit THEN 1 ELSE 0 END) AS cache_hit_count,
  SUM(CASE WHEN js.error_result IS NOT NULL THEN 1 ELSE 0 END) AS error_count,
  SUM(js.total_bytes_processed) AS total_bytes_processed,
  SUM(js.total_bytes_billed) AS total_bytes_billed,
  -- Calculate approximate cost (using the configurable cost per TB)
  ROUND(SUM(js.total_bytes_billed) / POWER(1024, 4) * ${dataform.projectConfig.vars.cost_per_terabyte}, 2) AS estimated_cost_usd,
  SUM(js.total_slot_ms) / 1000 / 3600 AS slot_hours,
  -- Calculate cache efficiency
  ROUND(SUM(CASE WHEN js.cache_hit THEN 1 ELSE 0 END) / NULLIF(COUNT(*), 0) * 100, 2) AS cache_hit_percentage,
  -- Add array of datasets and their costs
  ARRAY(
    SELECT AS STRUCT 
      dc.dataset, 
      SUM(dc.bytes_processed) AS bytes_processed,
      SUM(dc.bytes_billed) AS bytes_billed,
      SUM(dc.dataset_cost_usd) AS dataset_cost_usd
    FROM dataset_costs dc
    WHERE 
      dc.date = date -- Use the grouped date field
      AND dc.project_id = project_id -- Use the grouped project_id field
      AND dc.user_email = user_email -- Use the grouped user_email field  
      AND (dc.service_account = service_account OR (dc.service_account IS NULL AND service_account IS NULL))
    GROUP BY dc.dataset
    ORDER BY dataset_cost_usd DESC
  ) AS dataset_costs
FROM
  job_stats js
GROUP BY
  date, project_id, user_email, service_account
ORDER BY
  date DESC, estimated_cost_usd DESC
