config {
  type: "table",
  description: "Enhanced table containing BigQuery usage and cost data with granular insights",
  columns: {
    date: "Date of the query execution (YYYY-MM-DD)",
    project_id: "Google Cloud project ID",
    user_email: "Email of the user who executed the query",
    service_account: "Service account identifier if the query was executed by a service account",
    query_count: "Number of queries executed",
    cache_hit_count: "Number of queries that used cached results",
    error_count: "Number of queries that resulted in errors",
    total_bytes_processed: "Total bytes processed by all queries",
    total_bytes_billed: "Total bytes billed for all queries",
    estimated_cost_usd: "Estimated cost in USD based on bytes billed",
    slot_hours: "Total slot hours consumed",
    cache_hit_percentage: "Percentage of queries that used cached results",
    hourly_breakdown: "Array containing query count and cost breakdowns by hour of day",
    daily_breakdown: "Array containing query count and cost breakdowns by day of week",
    dataset_costs: "Array of datasets used with their associated costs",
    recent_queries: "Array of recent queries with detailed information for analysis"
  },
  bigquery: {
    partitionBy: "date",
    clusterBy: ["project_id", "user_email"]
  },
  tags: ["cost", "monitoring", "service-accounts", "datasets", "analytics"]
}

-- This query uses the cost_query.sql file with parameters
WITH job_stats AS (
  SELECT
    project_id,
    user_email,
    -- Extract service account info from user_email if present
    CASE 
      WHEN user_email LIKE '%.gserviceaccount.com' THEN user_email
      WHEN user_email LIKE 'service-%' THEN user_email
      ELSE NULL
    END AS service_account,
    job_id,
    creation_time,
    end_time,
    query,
    total_bytes_processed,
    total_bytes_billed,
    total_slot_ms,
    error_result,
    cache_hit,
    destination_table,
    referenced_tables,
    labels,
    -- Extract dataset information from referenced tables
    ARRAY(
      SELECT DISTINCT 
        CONCAT(ref_table.project_id, '.', ref_table.dataset_id)
      FROM 
        UNNEST(referenced_tables) AS ref_table
      WHERE 
        ref_table.project_id IS NOT NULL 
        AND ref_table.dataset_id IS NOT NULL
    ) AS referenced_datasets
  FROM
    ${ref("INFORMATION_SCHEMA.JOBS")}
  WHERE
    creation_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL ${dataform.projectConfig.vars.history_days} DAY)
    AND job_type = 'QUERY'
    AND statement_type != 'SCRIPT'
),

-- Create a date dimension for each job to avoid GROUP BY issues
job_dates AS (
  SELECT
    job_id,
    FORMAT_TIMESTAMP('%Y-%m-%d', creation_time) AS date,
    project_id,
    user_email,
    service_account
  FROM job_stats
),

-- Calculate per-dataset costs by attributing the cost of each query proportionally
dataset_costs AS (
  SELECT
    jd.date,
    jd.project_id,
    jd.user_email,
    jd.service_account,
    dataset,
    -- Count queries referencing this dataset
    COUNT(*) AS query_count,
    -- Sum bytes processed, attributed proportionally if multiple datasets are involved
    SUM(js.total_bytes_processed / ARRAY_LENGTH(js.referenced_datasets)) AS bytes_processed,
    SUM(js.total_bytes_billed / ARRAY_LENGTH(js.referenced_datasets)) AS bytes_billed,
    -- Calculate approximate dataset cost (using the configurable cost per TB)
    ROUND(SUM(js.total_bytes_billed / ARRAY_LENGTH(js.referenced_datasets)) / 
          POWER(1024, 4) * ${dataform.projectConfig.vars.cost_per_terabyte}, 2) AS dataset_cost_usd
  FROM
    job_dates jd
  JOIN 
    job_stats js ON jd.job_id = js.job_id,
    UNNEST(js.referenced_datasets) AS dataset
  WHERE
    ARRAY_LENGTH(js.referenced_datasets) > 0
  GROUP BY
    jd.date, jd.project_id, jd.user_email, jd.service_account, dataset
)

-- Main query with user and service account attribution
SELECT
  jd.date,
  jd.project_id,
  jd.user_email,
  jd.service_account,
  -- Query stats
  COUNT(*) AS query_count,
  SUM(CASE WHEN js.cache_hit THEN 1 ELSE 0 END) AS cache_hit_count,
  SUM(CASE WHEN js.error_result IS NOT NULL THEN 1 ELSE 0 END) AS error_count,
  SUM(js.total_bytes_processed) AS total_bytes_processed,
  SUM(js.total_bytes_billed) AS total_bytes_billed,
  -- Calculate approximate cost (using the configurable cost per TB)
  ROUND(SUM(js.total_bytes_billed) / POWER(1024, 4) * ${dataform.projectConfig.vars.cost_per_terabyte}, 2) AS estimated_cost_usd,
  SUM(js.total_slot_ms) / 1000 / 3600 AS slot_hours,
  -- Calculate cache efficiency
  ROUND(SUM(CASE WHEN js.cache_hit THEN 1 ELSE 0 END) / NULLIF(COUNT(*), 0) * 100, 2) AS cache_hit_percentage,
  -- Pre-aggregated dataset costs using a join pattern instead of a correlated subquery
  (
    SELECT ARRAY_AGG(
      STRUCT(
        agg_dc.dataset, 
        agg_dc.bytes_processed,
        agg_dc.bytes_billed,
        agg_dc.dataset_cost_usd
      )
      ORDER BY agg_dc.dataset_cost_usd DESC
    )
    FROM (
      SELECT
        dc.date,
        dc.project_id,
        dc.user_email,
        dc.service_account,
        dc.dataset,
        SUM(dc.bytes_processed) AS bytes_processed,
        SUM(dc.bytes_billed) AS bytes_billed,
        SUM(dc.dataset_cost_usd) AS dataset_cost_usd
      FROM dataset_costs dc
      GROUP BY
        dc.date, dc.project_id, dc.user_email, dc.service_account, dc.dataset
    ) agg_dc
    WHERE 
      agg_dc.date = jd.date
      AND agg_dc.project_id = jd.project_id
      AND agg_dc.user_email = jd.user_email
      AND (agg_dc.service_account = jd.service_account OR (agg_dc.service_account IS NULL AND jd.service_account IS NULL))
  ) AS dataset_costs
FROM
  job_dates jd
JOIN
  job_stats js ON jd.job_id = js.job_id
GROUP BY
  jd.date, jd.project_id, jd.user_email, jd.service_account
ORDER BY
  jd.date DESC, estimated_cost_usd DESC
